{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6a276038-87a7-40be-b78a-0b17dbe288eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, time, os, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8bc5ef6c-cad7-444f-b808-9fa3692986da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6fa2098d-7c7f-4fe8-ae26-5d66b1862bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0bab195-6912-4d34-b744-910ab27962fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_eps = 1e-8\n",
    "\n",
    "def cov_matrix(X, eps=_eps):\n",
    "    # X: (N, D) numpy\n",
    "    if X.shape[0] <= 1:\n",
    "        return np.zeros((X.shape[1], X.shape[1]))\n",
    "    C = np.cov(X, rowvar=False)\n",
    "    C += np.eye(C.shape[0]) * eps\n",
    "    return C\n",
    "\n",
    "def cov_volume(X, eps=_eps):\n",
    "    # return product of eigenvalues (i.e., det), or sqrt(det) as volume\n",
    "    C = cov_matrix(X, eps=eps)\n",
    "    sign, logdet = np.linalg.slogdet(C)\n",
    "    if sign <= 0:\n",
    "        return 0.0\n",
    "    # \"volume\" = sqrt(det(C)) -> 0.5 * logdet\n",
    "    return float(np.exp(0.5 * logdet))\n",
    "\n",
    "def log_cov_volume(X, eps=_eps):\n",
    "    C = cov_matrix(X, eps=eps)\n",
    "    sign, logdet = np.linalg.slogdet(C)\n",
    "    if sign <= 0:\n",
    "        return np.nan\n",
    "    return 0.5 * logdet\n",
    "\n",
    "def fisher_ratio(acts, labels):\n",
    "    # acts: (N,D) np, labels: (N,) np\n",
    "    labels = np.array(labels)\n",
    "    classes = np.unique(labels)\n",
    "    overall_mean = acts.mean(axis=0)\n",
    "    D = acts.shape[1]\n",
    "    Sb = np.zeros((D,D))\n",
    "    Sw = np.zeros((D,D))\n",
    "    for c in classes:\n",
    "        Xc = acts[labels==c]\n",
    "        if Xc.shape[0] == 0: continue\n",
    "        mean_c = Xc.mean(axis=0)\n",
    "        Nc = Xc.shape[0]\n",
    "        diff = (mean_c - overall_mean).reshape(-1,1)\n",
    "        Sb += Nc * (diff @ diff.T)\n",
    "        if Xc.shape[0] > 1:\n",
    "            Sw += np.cov(Xc, rowvar=False) * (Nc-1)\n",
    "    # scalar fisher ratio (trace)\n",
    "    trSb = np.trace(Sb)\n",
    "    trSw = np.trace(Sw) + _eps\n",
    "    return trSb / trSw\n",
    "\n",
    "def avg_movement_between_layers(projected_acts):\n",
    "    # projected_acts: list of (N, k) np arrays, same N each\n",
    "    movements = []\n",
    "    for i in range(len(projected_acts)-1):\n",
    "        d = projected_acts[i+1] - projected_acts[i]\n",
    "        dists = np.linalg.norm(d, axis=1)\n",
    "        movements.append(dists.mean())\n",
    "    return movements\n",
    "\n",
    "# CKA (linear version) for representational similarity\n",
    "def linear_CKA(X, Y):\n",
    "    # X, Y: (N, D1) and (N, D2)\n",
    "    # center\n",
    "    X = X - X.mean(0, keepdims=True)\n",
    "    Y = Y - Y.mean(0, keepdims=True)\n",
    "    HSIC = np.linalg.norm(X.T @ Y, 'fro')**2\n",
    "    denom = (np.linalg.norm(X.T @ X, 'fro') * np.linalg.norm(Y.T @ Y, 'fro')) + _eps\n",
    "    return HSIC / denom\n",
    "\n",
    "# Cosine similarity between flattened weight matrices\n",
    "def weight_cosine_similarity(w1, w2):\n",
    "    a = w1.flatten()\n",
    "    b = w2.flatten()\n",
    "    min_len = min(len(a), len(b))\n",
    "    a = a[:min_len]\n",
    "    b = b[:min_len]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + _eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "857aaf43-36f7-4295-976b-5d6a61a85208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "def get_toy(dataset='moons', n=1000, noise=0.1):\n",
    "    if dataset == 'moons':\n",
    "        X, y = make_moons(n_samples=n, noise=noise, random_state=seed)\n",
    "    else:\n",
    "        X, y = make_circles(n_samples=n, noise=noise, factor=0.5, random_state=seed)\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    return X, y\n",
    "\n",
    "def get_mnist(subset=5000, flatten=True):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    # small subset for speed\n",
    "    indices = list(range(min(subset, len(mnist))))\n",
    "    loader = DataLoader(torch.utils.data.Subset(mnist, indices), batch_size=subset, shuffle=False)\n",
    "    imgs, labels = next(iter(loader))\n",
    "    if flatten:\n",
    "        imgs = imgs.view(imgs.size(0), -1)\n",
    "    return imgs.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "43695e22-aa04-49cb-b6d1-045ebd9a9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, depth, activation='tanh', bias=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if depth == 1:\n",
    "            layers.append(nn.Linear(in_dim, out_dim, bias=bias))\n",
    "        else:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            for _ in range(depth-2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim, bias=bias))\n",
    "            layers.append(nn.Linear(hidden_dim, out_dim, bias=bias))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        if activation == 'tanh':\n",
    "            self.act = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.act = F.relu\n",
    "        else:\n",
    "            self.act = torch.tanh\n",
    "\n",
    "    def forward(self, x, return_activations=False):\n",
    "        acts = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # apply nonlinearity on all but final\n",
    "            if i != len(self.layers)-1:\n",
    "                x = self.act(x)\n",
    "            if return_activations:\n",
    "                acts.append(x)   # store post-nonlinearity for intermediate, store logits for final\n",
    "        if return_activations:\n",
    "            return x, acts\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e801c3e8-a722-406d-9457-63987cfd11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train(model, X, y, epochs=20, lr=1e-3, batch_size=None, verbose=False):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if batch_size is None:\n",
    "        batch_size = X.shape[0]\n",
    "    ds = TensorDataset(X, y)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        correct = 0\n",
    "        loss_acc = 0.0\n",
    "        for xb, yb in loader:\n",
    "            opt.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = F.cross_entropy(out, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            loss_acc += float(loss.item()) * xb.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "        if verbose and (ep%5==0 or ep==epochs-1):\n",
    "            print(f\"Ep {ep+1}/{epochs} loss={loss_acc/total:.4f} acc={correct/total:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a4194b56-9a65-4a29-b9e9-15262aee0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, X):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, acts = model(X, return_activations=True)\n",
    "    # acts is list of tensors (N,D_l)\n",
    "    acts_cpu = [a.detach().cpu().float() for a in acts]\n",
    "    return acts_cpu\n",
    "\n",
    "def project_activations_consistent(acts_list, n_components=2):\n",
    "    \"\"\"\n",
    "    Acts_list: list of (N, D_l) tensors.\n",
    "    We return projected list in same PCA basis by fitting PCA on concatenated rows\n",
    "    AFTER padding features via PCA per-layer? Approach:\n",
    "      - For stability across layers, we compute per-layer PCA to reduce to min(n_components, D_l),\n",
    "        then further align via PCA fitted on concatenation of the reduced coordinates (if dims match).\n",
    "    Simpler robust approach implemented here: per-layer PCA to k dims (k=min(n_components, D_l)),\n",
    "    and if k==n_components for all layers, we return list of arrays with same dims.\n",
    "    Otherwise we pad smaller dims with zeros (conservative) so we can compute movements.\n",
    "    \"\"\"\n",
    "    acts_np = [a.numpy() for a in acts_list]\n",
    "    ks = [min(n_components, a.shape[1]) for a in acts_np]\n",
    "    reduced = []\n",
    "    for a, k in zip(acts_np, ks):\n",
    "        if k <= 0:\n",
    "            reduced.append(np.zeros((a.shape[0], n_components)))\n",
    "            continue\n",
    "        pca = PCA(n_components=k)\n",
    "        red = pca.fit_transform(a)\n",
    "        if k < n_components:\n",
    "            # pad with zeros on right to reach n_components\n",
    "            red = np.hstack([red, np.zeros((a.shape[0], n_components-k))])\n",
    "        reduced.append(red)\n",
    "    return reduced  # list of (N, n_components) arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "098af8d1-2604-42f6-9f6d-b3a5256f8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model, X, y, n_components=3, do_project=True, eps=_eps):\n",
    "    \"\"\"\n",
    "    returns a dict with:\n",
    "      - per-layer means, variances\n",
    "      - per-layer covariance volumes (per class)\n",
    "      - per-layer log-cov volumes (per class)\n",
    "      - per-layer fisher ratios\n",
    "      - projected activations (for movement)\n",
    "      - per-layer movement magnitudes\n",
    "      - weight similarities (pairwise cosine between sequential layers)\n",
    "      - CKA matrix (pairwise CKA between layers)\n",
    "      - eigen spectra top-K per layer per class (optional)\n",
    "    \"\"\"\n",
    "    acts = get_activations(model, X)\n",
    "    N = X.shape[0]\n",
    "    labels_np = y.cpu().numpy()\n",
    "    results = {}\n",
    "    # per-layer moment stats\n",
    "    means = [a.numpy().mean(axis=0) for a in acts]\n",
    "    variances = [a.numpy().var(axis=0) for a in acts]\n",
    "    results['means'] = means\n",
    "    results['variances'] = variances\n",
    "\n",
    "    # per-class volumes & log-volumes\n",
    "    classes = np.unique(labels_np)\n",
    "    vols = {int(c): [] for c in classes}\n",
    "    logvols = {int(c): [] for c in classes}\n",
    "    fisher = []\n",
    "    for a in acts:\n",
    "        a_np = a.numpy()\n",
    "        for c in classes:\n",
    "            Xc = a_np[labels_np==c]\n",
    "            if Xc.shape[0] < 2:\n",
    "                vols[int(c)].append(np.nan)\n",
    "                logvols[int(c)].append(np.nan)\n",
    "            else:\n",
    "                vols[int(c)].append(cov_volume(Xc, eps=eps))\n",
    "                logvols[int(c)].append(log_cov_volume(Xc, eps=eps))\n",
    "        # fisher ratio\n",
    "        fisher.append(fisher_ratio(a_np, labels_np))\n",
    "    results['volumes'] = vols\n",
    "    results['logvolumes'] = logvols\n",
    "    results['fisher'] = fisher\n",
    "\n",
    "    # projected activations & movements\n",
    "    if do_project:\n",
    "        projected = project_activations_consistent(acts, n_components=n_components)\n",
    "        movements = avg_movement_between_layers(projected)\n",
    "        results['projected'] = projected\n",
    "        results['movements'] = movements\n",
    "\n",
    "    # weight-based redundancy: cosine similarity between sequential layer weights\n",
    "    weights = [p.detach().cpu().numpy() for n,p in model.named_parameters() if 'weight' in n]\n",
    "    # flatten and compute pairwise sequential cosine\n",
    "    wcos = []\n",
    "    for i in range(len(weights)-1):\n",
    "        wcos.append(weight_cosine_similarity(weights[i], weights[i+1]))\n",
    "    results['weight_cosine_seq'] = wcos\n",
    "\n",
    "    # CKA matrix between layer activations\n",
    "    num_layers = len(acts)\n",
    "    CKA = np.zeros((num_layers, num_layers))\n",
    "    for i in range(num_layers):\n",
    "        for j in range(num_layers):\n",
    "            CKA[i,j] = linear_CKA(acts[i].numpy(), acts[j].numpy())\n",
    "    results['CKA'] = CKA\n",
    "\n",
    "    # between/within traces\n",
    "    tb, tw = [], []\n",
    "    for a in acts:\n",
    "        a_np = a.numpy()\n",
    "        # re-use fisher helper via Sb,Sw traces\n",
    "        # compute Sb and Sw traces manually\n",
    "        overall = a_np.mean(axis=0)\n",
    "        Sb = np.zeros((a_np.shape[1], a_np.shape[1]))\n",
    "        Sw = np.zeros_like(Sb)\n",
    "        for c in classes:\n",
    "            Xc = a_np[labels_np==c]\n",
    "            if Xc.shape[0] == 0: continue\n",
    "            mean_c = Xc.mean(axis=0)\n",
    "            Nc = Xc.shape[0]\n",
    "            diff = (mean_c - overall).reshape(-1,1)\n",
    "            Sb += Nc * (diff @ diff.T)\n",
    "            if Xc.shape[0] > 1:\n",
    "                Sw += np.cov(Xc, rowvar=False) * (Nc-1)\n",
    "        tb.append(np.trace(Sb))\n",
    "        tw.append(np.trace(Sw))\n",
    "    results['trace_between'] = tb\n",
    "    results['trace_within'] = tw\n",
    "\n",
    "    print(\"RESULTS\\n\")\n",
    "    print(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "27a1972b-73a0-4a12-bbd0-27eef2f6de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_toy(dataset='moons', depth_list=[3,6], hidden=12, epochs=40, quick=True):\n",
    "    X,y = get_toy(dataset=dataset, n=1000, noise=0.12)\n",
    "    X = X.to(device); y = y.to(device)\n",
    "    summary = {}\n",
    "    for depth in depth_list:\n",
    "        print(f\"Running depth={depth} ...\")\n",
    "        model = MLP(in_dim=2, hidden_dim=hidden, out_dim=2, depth=depth, activation='tanh').to(device)\n",
    "        model = quick_train(model, X, y, epochs=epochs if not quick else max(5, epochs//8), lr=1e-3, verbose=True)\n",
    "        res = analyze_model(model, X, y, n_components=3, do_project=True)\n",
    "        summary[depth] = res\n",
    "    return X, y, summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e4954c66-ce0b-46e9-9c94-5f2d90f68c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example quick run (toy)\n",
    "# Xtoy, ytoy, toy_summary = run_experiment_toy(dataset='moons', depth_list=[3,6], hidden=6, epochs=40, quick=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
